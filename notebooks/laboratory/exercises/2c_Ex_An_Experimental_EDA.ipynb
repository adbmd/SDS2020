{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2c_Ex_An_Experimental_EDA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudio1975/SDS2020/blob/master/notebooks/laboratory/exercises/2c_Ex_An_Experimental_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlcrvwrX70Uv",
        "colab_type": "text"
      },
      "source": [
        "# **An Experimental Exploratory Data Analysis for a Classification Task step 2**\n",
        "\n",
        "### ***From Visualization to Statistical Analysis***\n",
        "\n",
        "### ***From Feature Engineering to Feature Selection***\n",
        "\n",
        "### ***From the Best Model Selection to Interpretability***\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vngHrhnzA7qr",
        "colab_type": "text"
      },
      "source": [
        "To start the exploration set up the environment with libraries, upload the data set (it's stored in a github repository) and split it into target variable and features variables. No more set up is required using Google Colab. Look at the guidelines: https://colab.research.google.com/notebooks/welcome.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2o9mujoz5by",
        "colab_type": "text"
      },
      "source": [
        "#### **Contents**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRwDUZ019MxF",
        "colab_type": "text"
      },
      "source": [
        "The goal of this challenge, launched by CrowdAnalytix, is to develop a model to predict whether a mortgage will be funded or not based on certain factors in a customerâ€™s application data. \n",
        "The evaluation metric used is the F1 score.\n",
        "The data set is made up by 45.642 observations with predictor variables (21 features) and the target variable. It's a classification task with the goal to predict the 'Result' target variable for every row (Funded, Not Funded). Look at the competition: https://www.crowdanalytix.com/contests/propensity-to-fund-mortgages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9DkyQZ2Ls-4",
        "colab_type": "text"
      },
      "source": [
        "### **Exploratory Data Analysis (EDA) Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVQahUb-8995",
        "colab_type": "text"
      },
      "source": [
        "![](http://www.theleader.info/wp-content/uploads/2017/07/Mortgage-rates.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs9lEhRM0Mne",
        "colab_type": "text"
      },
      "source": [
        "# Prepare Workspace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJlI9RQG-A8w",
        "colab_type": "text"
      },
      "source": [
        "#####- Upload libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQysnLU9RUKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload libraries\n",
        "\n",
        "# to handle data set\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# to plot\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "# statistics\n",
        "import statistics\n",
        "import scipy.stats as stats\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import logit\n",
        "from scipy.stats import chi2_contingency\n",
        "from scipy.stats import kurtosis \n",
        "from scipy.stats import skew\n",
        "from statistics import stdev \n",
        "\n",
        "# to split data set \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# standardization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# to build models\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# to evaluate models\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# to handle imbalanced data set\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from collections import Counter\n",
        "\n",
        "# feature engineering\n",
        "!pip install feature-engine\n",
        "import feature_engine\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "# feature importance\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "!pip install eli5 \n",
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "!pip install shap\n",
        "import shap\n",
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLFLLtY2-I06",
        "colab_type": "text"
      },
      "source": [
        "#####- Upload data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgQMY8HqDrGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload dataset\n",
        "url = 'https://raw.githubusercontent.com/claudio1975/SDS2020/master/data/CAX_train_small.csv'\n",
        "df = pd.read_csv(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2_fumrJ-NCl",
        "colab_type": "text"
      },
      "source": [
        "#####- Split data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwPBNz5pDvDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split data set between target and features\n",
        "X_full = df\n",
        "y = X_full.RESULT\n",
        "X_full = X_full.drop(['RESULT'], axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIEjdLM1BGti",
        "colab_type": "text"
      },
      "source": [
        "# Summarize Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvXmGWII7I_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at dimension of data set and types of each attribute\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDB8l79ZLldm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Summarize attribute distributions of the data frame\n",
        "df.describe(include='all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl_waIVssaPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Take a peek at the first rows of the data\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6N4eR9vEXQi",
        "colab_type": "text"
      },
      "source": [
        "Explanatory variables are grouped into categorical variables and numerical variables and for each one let's do a graphical and non-graphical analysis, but before this split let's run some some data preparation activities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqhoHvdpPKse",
        "colab_type": "text"
      },
      "source": [
        "# Formatting Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbHzfBaj2J5t",
        "colab_type": "text"
      },
      "source": [
        "If necessary, it's a good practice to format data, after have taken a peek of it. Missing values on numeric features are marked by \"-1\", meanwhile for categorical features they are marked with \"Unknown\"; let's imput these values with \"NA\".  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoC1p8FWLFbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Replaced both '-1' and 'Unknown' values with NA's\n",
        "X_full[X_full== -1] = np.nan\n",
        "X_full[X_full==\"Unknown\"] = np.nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cfr3gw7vQeWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Format data into float and object types and split mixed variables\n",
        "X_full['PROPERTY VALUE'] = X_full['PROPERTY VALUE'].astype(float)\n",
        "X_full['MORTGAGE PAYMENT'] = X_full['MORTGAGE PAYMENT'].astype(float)\n",
        "X_full['AMORTIZATION'] = X_full['AMORTIZATION'].astype(float)\n",
        "X_full['TERM'] = X_full['TERM'].astype(float)\n",
        "X_full['INCOME'] = X_full['INCOME'].astype(float)\n",
        "X_full['INCOME TYPE'] = X_full['INCOME TYPE'].astype(object)\n",
        "X_full['CREDIT SCORE'] = X_full['CREDIT SCORE'].astype(float)\n",
        "X_full['FSA_num'] = X_full['FSA'].str.extract('(\\d+)') # extract numerical part\n",
        "X_full['FSA_let'] = X_full['FSA'].str[0] # extract the first letter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa67IEEHGdPg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Rename some features for a practical use\n",
        "X_full = X_full.rename(columns={\"MORTGAGE PURPOSE\":\"MORTGAGE_PURPOSE\",\"PAYMENT FREQUENCY\":\"PAYMENT_FREQUENCY\",\"PROPERTY TYPE\":\"PROPERTY_TYPE\",\"AGE RANGE\":\"AGE_RANGE\",\"PROPERTY VALUE\": \"PROPERTY_VALUE\",\n",
        "                                \"MORTGAGE PAYMENT\": \"MORTGAGE_PAYMENT\", \"MORTGAGE AMOUNT\":\"MORTGAGE_AMOUNT\",\"INCOME TYPE\":\"INCOME_TYPE\",\"CREDIT SCORE\":\"CREDIT_SCORE\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfJuPqA7ohVw",
        "colab_type": "text"
      },
      "source": [
        "# Handling Missing Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQO8Om84omdg",
        "colab_type": "text"
      },
      "source": [
        "There are two categorical features with missing values lower than 40%. The approach followed: filled up missing values with random sampling. With large percentage of missing values (>=15%) it's suggested to add a \"missing indicator\", a boolean variable with 1/true (missing value) or 0/false (actual value). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8O3lnuJotQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check missing values both to numeric features and categorical features\n",
        "X_full.isnull().sum()/X_full.shape[0]*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DT3uzfUb1ymn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the new variable where NA will be imputed\n",
        "X_full['GENDER_imputed'] = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odssE6_Z2Gc-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extract the random sample to fill the na\n",
        "random_sample_X_full ="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAh1b_HH2KFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pandas needs to have the same index in order to merge datasets\n",
        "random_sample_X_full.index = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRsVpTOL2oef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input missing values in the newly created variable\n",
        "X_full.loc[X_full['GENDER'].isnull(), 'GENDER_imputed'] = random_sample_X_full"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AuVzCtt82Tr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the new variable where NA will be imputed\n",
        "X_full['INCOME_TYPE_imputed'] = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Am2fPNW88vs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extract the random sample to fill the na\n",
        "random_sample_X_full = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5W-H4_a8-YV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pandas needs to have the same index in order to merge datasets\n",
        "random_sample_X_full.index = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iytyjy8w9FR_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input the missing values in the newly created variable\n",
        "X_full.loc[X_full['INCOME_TYPE'].isnull(), 'INCOME_TYPE_imputed'] = random_sample_X_full"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ji_SDLc79csd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# drop the original features\n",
        "X_full = X_full.drop(['GENDER', 'INCOME_TYPE'],axis=1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt27Gpkfo32t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# final check\n",
        "X_full.isnull().sum()/X_full.shape[0]*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqcTsDB4OkMK",
        "colab_type": "text"
      },
      "source": [
        "# Handling Categorical Features\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsdKWS7-o6a_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's have a look at how many labels for categorical features\n",
        "for col in X_full.columns:\n",
        "  if X_full[col].dtype ==\"object\":\n",
        "    print(col, ': ', len(X_full[col].unique()), ' labels')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NREoC4kEBP17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
        "categorical_cols = [cname for cname in X_full.columns if\n",
        "                    X_full[....].nunique() <= 15 and \n",
        "                    X_full[....].dtype == \"object\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfZUySHMrLiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Subset with categorical features\n",
        "cat = X_full[categorical_cols]\n",
        "cat.columns\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9notscfuyPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHJVRnRmT7-j",
        "colab_type": "text"
      },
      "source": [
        "#####- Feature Engineering on categorical features: target encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSvpWEpNMQuJ",
        "colab_type": "text"
      },
      "source": [
        "Let's transform categorical features into numerical variables with target encoding methodology to afford a better understanding of variables by machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HdFaBQ8L-CZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Merge categorical covariates with dependent variable\n",
        "cat2 = pd.concat([cat,y],axis=1)\n",
        "cat2['RESULT'] = np.where(cat2['RESULT']=='FUNDED',1,0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPH7aZWZK57v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate the mean target value per category for each feature and capture the result in a dictionary \n",
        "MORTGAGE_PURPOSE_LABELS = cat2.groupby(['MORTGAGE_PURPOSE'])['RESULT'].mean().to_dict()\n",
        "PAYMENT_FREQUENCY_LABELS = cat2.groupby(['PAYMENT_FREQUENCY'])['RESULT'].mean().to_dict()\n",
        "PROPERTY_TYPE_LABELS = cat2.groupby(['PROPERTY_TYPE'])['RESULT'].mean().to_dict()\n",
        "AGE_RANGE_LABELS = cat2.groupby(['AGE_RANGE'])['RESULT'].mean().to_dict()\n",
        "GENDER_LABELS = cat2.groupby(['GENDER_imputed'])['RESULT'].mean().to_dict()\n",
        "FSA_num_LABELS = cat2.groupby(['FSA_num'])['RESULT'].mean().to_dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlBmMj5gMt7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# replace for each feature the labels with the mean target values\n",
        "cat2['MORTGAGE_PURPOSE'] = cat2['MORTGAGE_PURPOSE'].map(....)\n",
        "cat2['PAYMENT_FREQUENCY'] = cat2['PAYMENT_FREQUENCY'].map(....)\n",
        "cat2['PROPERTY_TYPE'] = cat2['PROPERTY_TYPE'].map(......)\n",
        "cat2['AGE_RANGE'] = cat2['AGE_RANGE'].map(.....)\n",
        "cat2['GENDER_imputed'] = cat2['GENDER_imputed'].map(.....)\n",
        "cat2['FSA_num'] = cat2['FSA_num'].map(.....)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nsr5D7xTHNdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at the new subset\n",
        "target_cat = cat2.drop(['RESULT'], axis=1)\n",
        "target_cat.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pUe0kjijI5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_cat.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NC7-Ueorbvq",
        "colab_type": "text"
      },
      "source": [
        "# Numerical Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_0FZBcsU8eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select numerical columns\n",
        "numerical_cols = [cname for cname in X_full.columns if \n",
        "                X_full[cname].dtype in ['float64']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fg-ru4uxU88C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Subset with numerical features\n",
        "num = X_full[numerical_cols]\n",
        "num.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPeULBW3U9Fo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSRSSCLGVEh5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Grasp all\n",
        "X_all = pd.concat([target_cat, num], axis=1, join='inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3g7VOS0w2lD",
        "colab_type": "text"
      },
      "source": [
        "# Split data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_Hw64zwxu_6",
        "colab_type": "text"
      },
      "source": [
        "To analyze the performance of a model is a good manner to split the data set into the training set and the test set. It's been decided to split it into three parts: training set, validation set and test set for a better understanding of models. The training set is a sample of data used to fit the model, meanwhile the validation set is a sample of data used to provide an unbiased evaluation of the model that fit on the training set and to tune the model hyperparameters (not in this explorative phase). The test set is a sample of data used to provide an unbiased evaluation of the model applied on data never seen before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csVOGV7Qw5VG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Break off validation and test set from training data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y, train_size=0.8, test_size=0.2,\n",
        "                                                                random_state=0)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, train_size=0.8, test_size=0.2,\n",
        "                                                                random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tfvk77arw9dH",
        "colab_type": "text"
      },
      "source": [
        "# Standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geBXV2Ah45YO",
        "colab_type": "text"
      },
      "source": [
        "Since values of the features are not uniform and may be neagatively impact the skill of some models, the same models are evaluated with a standardized copy of the data set. It means, data are transformed such that each feature has a mean value of 0 and a standard deviation of 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLARBi9ow-OD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standardization of data\n",
        "sc = StandardScaler()\n",
        "X_train_sc = sc.fit_transform(X_train)\n",
        "X_valid_sc = sc.fit_transform(X_valid)\n",
        "X_test_sc = sc.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe16GLXqopQl",
        "colab_type": "text"
      },
      "source": [
        "# Modeling Part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5VfzCYGcWgq",
        "colab_type": "text"
      },
      "source": [
        "The traditional data exploration is extended looking at the behaviour of several baseline models and which features can be relevant for the prediction. This exploration is splitted in two parts: without handling the imbalanced target variable (scaled baseline models) and handling it (scaled baseline models).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S3Jt8sxLF2b",
        "colab_type": "text"
      },
      "source": [
        "- Evaluation Metric and Confusion Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zebre5UP0u7",
        "colab_type": "text"
      },
      "source": [
        "The confusion matrix is a summary table representation of prediction results for a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class. The diagonal elements represent the number of points for which the predicted label is equal to the true label, while off-diagonal elements are those that are mislabeled by the classifier. Good predictions coming from the higher diagonal values of the confusion matrix. For this imbalanced classification task is not used Accuracy metric but more appropriately the F1 score metric that combines both precision and recall, it's an harmonic mean between them, it's indicates how precise is the classifier (precision) and how robust it is (recall). F1 score equal to 0.00 indicates a poor model, instead F1 score equal 1.00 indicates a perfect model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQWGVT8QwwI8",
        "colab_type": "text"
      },
      "source": [
        "#  Modeling Part I: without handling imbalanced data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fHQdwCHxrIL",
        "colab_type": "text"
      },
      "source": [
        "The analysis is based on six baseline models: Logistic Regression as the easiest model and as well as benchmark, then other five models: Bagging, Random Forest, AdaBoost, Gradient Boosting Machine and Neural Networks (MLP)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwCD7aCLxEc6",
        "colab_type": "text"
      },
      "source": [
        "#####- Baseline Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d8vIEE4xFMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Spot Check Algorithms\n",
        "models = []\n",
        "models.append(('LogisticRegression', LogisticRegression(random_state=0)))\n",
        "models.append(('Bagging', BaggingClassifier(random_state=0)))\n",
        "models.append(('RandomForest', RandomForestClassifier(random_state=0)))\n",
        "models.append(('AdaBoost', AdaBoostClassifier(random_state=0)))\n",
        "models.append(('GBM', GradientBoostingClassifier(random_state=0)))\n",
        "models.append(('NN', MLPClassifier(random_state=0)))\n",
        "results_tr = []\n",
        "results_v = []\n",
        "results_t = []\n",
        "names = []\n",
        "score = []\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "for (name, model) in models:\n",
        "    param_grid = {}\n",
        "    my_model = GridSearchCV(model,param_grid,cv=skf)\n",
        "    my_model.fit(.....)\n",
        "    predictions_tr = my_model.predict(X_train_sc) \n",
        "    predictions_v = my_model.predict(X_valid_sc)\n",
        "    predictions_t = my_model.predict(X_test_sc)\n",
        "    f1_train = f1_score(y_train, predictions_tr, average='macro') \n",
        "    f1_valid = f1_score(y_valid, predictions_v,average='macro') \n",
        "    f1_test = f1_score(y_test, predictions_t,average='macro') \n",
        "    results_tr.append(f1_train)\n",
        "    results_v.append(f1_valid)\n",
        "    results_t.append(f1_test)\n",
        "    \n",
        "    names.append(name)\n",
        "    f_dict = {\n",
        "        'model': name,\n",
        "        'f1_train': f1_train,\n",
        "        'f1_valid': f1_valid,\n",
        "        'f1_test': f1_test\n",
        "    }\n",
        "    score.append(f_dict)\n",
        "    # Computing Confusion matrix for the above algorithms\n",
        "    sns.set( rc = {'figure.figsize': (5, 5)})\n",
        "    plt.figure()\n",
        "    plot_confusion_matrix(my_model,X_test_sc, y_test,values_format= '.2f', cmap='Blues')\n",
        "    plt.title(name)\n",
        "    plt.show()   \n",
        "score = pd.DataFrame(score, columns = ['model',......])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyNrSed6x_68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at the F1 score for each model and for each data set\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBOAoq8kyDJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot results for a graphical comparison\n",
        "print(\"Spot Check Algorithms\")\n",
        "sns.set( rc = {'figure.figsize': (15, 5)})\n",
        "plt.figure()\n",
        "plt.subplot(1,3,1)  \n",
        "sns.stripplot(x=\"model\", y=\"f1_train\",data=score,size=15)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Train results')\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0,1.1])\n",
        "plt.subplot(1,3,2)\n",
        "sns.stripplot(x=\"model\", y=\"f1_valid\",data=score,size=15)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Validation results')\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0,1.1])\n",
        "plt.subplot(1,3,3)\n",
        "sns.stripplot(x=\"model\", y=\"f1_test\",data=score,size=15)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Test results')\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0,1.1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}